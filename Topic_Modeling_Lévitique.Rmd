---
title: "Topic_Modeling_L√©vitique"
author: "Alice Lefla√´c"
date: "F√©vrier 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Pr√©paration des donn√©es

 1.1 D√©finition de la session de travail
# Indication du chemin vers le notebook
```{r}
setwd("~/Documents/Alice (ordinateur Maman)/Humanit√©s Num√©riques/Topic Modeling")
monDossier="~/Documents/Alice (ordinateur Maman)/Humanit√©s Num√©riques/Topic Modeling"
```

# R√©cup√©ration des textes
Les textes latins bruts ont pr√©alablement √©t√© t√©l√©charg√©s en format .txt dans le dossier de travail.
    Le texte de la *Vulgate* a √©t√© r√©cup√©r√© sur le site BibleGateway (https://www.biblegateway.com/passage/?search=Leviticus%201&version=VULGATE) puis copi√© dans un √©diteur de texte (Sublime Text) et enregistr√© en format .txt.
    Le texte de l'*Heptateuque* a directement √©t√© r√©cup√©r√© en format .txt √† l'adresse suivante : https://scaife.perseus.org/reader/urn:cts:latinLit:stoa0104c.stoa001.opp-lat1:leviticus?q=leviticus&qk=form

```{r}
Leu_Vulgate <-readLines("Textes/Vulgate_Leu_all.txt")
# Fonction readLines indique le contenu de la variable sous forme de cha√Æne de caract√®res.
Leu_Hept <-readLines("Textes/Hept_Leu.txt")
```

 1.2 Premier nettoyage du texte : suppression de la ponctuation
On proc√®de √† un premier nettoyage du texte avant la lemmatisation.
# Suppression de la ponctuation

Cr√©ation d'un dossier Clearer pour ranger les textes plus propres.
#dir.create dans une cellule R ou cr√©ation d'un dossier dans les Documents de l'ordinateur.

Utilisation d'une fonction pour √©liminer la ponctuation encombrante avant la lemmatisation (la fonction removePunctation est difficilement exploitable dans le dataframe, il est plus simple de l'utiliser avant). Il faut installer la library tm.
```{r}
if(!require("tm")){
  install.packages("tm")
  library("tm")
}
#On √©limine la ponctuation du texte.
Leu_Vulgate_clearer <- removePunctuation(Leu_Vulgate)
Leu_Hept_clearer <- removePunctuation(Leu_Hept)
#Comme la ponctuation sera enlev√©e sans sauvegarder le r√©sultat, il faut cr√©er un nouveau fichier qui ira dans Clearer.
write(Leu_Vulgate_clearer, file = "Clearer/Leu_Vulgate_clearer.txt")
# On indique le nom du fichier et le chemin pour la nouvelle variable.
write(Leu_Hept_clearer, file = "Clearer/Leu_Hept_clearer.txt")
```

Les documents peuvent maintenant √™tre lemmatis√©s sur Pyrrha et export√©s en format .tsv. Un convertisseur de .tsv en .csv en ligne est ensuite utilis√©.

 1.3 Second nettoyage du texte : retrait des _stopwords_

 1.3.1 Importation de la liste de _stopwords_
 
```{r}
StopWords <- "StopwordsLatin(actualis√©).txt"
Stops = read.csv(StopWords, header=FALSE, stringsAsFactors=FALSE)[,]
head(Stops,10)
```

 1.3.2 Nettoyage du texte
 
Il est n√©cessaire de cr√©er une cha√Æne de caract√®res.
#Pour la Vulgate
```{r}
df_Vulgate <- read.csv("Leu_Vulgate_clear.csv", sep=",")
#Cr√©ation d'une cha√Æne de caract√®re vide qui contiendra √† l'avenir tous les textes contenus dans df. 
#[Dans le cours de Simon, la colomme Lemma est d√©j√† une cha√Æne de caract√®res. On ruse donc pour obtenir cette cha√Æne (elle est vide pour l'instant mais on dira √† R : si le mot n'appara√Æt pas dans les stopwords ou dans la ponctuation, tu le mets dans cette cha√Æne de caract√®res).]
cha√Æne_de_caract√®res_Vulgate <- ""
#Laisser le contenu vide : il n'y a rien pour l'instant.
```

#Pour l'Heptateuque
```{r}
df_Heptateuque <- read.csv("Leu_Hept_clear.csv", sep=",")
cha√Æne_de_caract√®res_Heptateuque <- ""
#Laisser le contenu vide : il n'y a rien pour l'instant.
cha√Æne_de_caract√®res_Vulgate
#Taper le nom de la variable me permet de l'afficher.
```

Puis on indique le contenu de la cha√Æne de caract√®res

# R√©duction √† la minuscule et retrait des stopwords 
On indique qu'on r√©duit √† la minuscule pour chaque mot √† l'aide d'une boucle, qu'on ne prend pas en compte les stopwords puis qu'on √¥te la ponctuation (en r√©alit√© la ponctuation avait √©t√© enlev√©e pr√©c√©demment, mais je laisse le code ici pour le garder en m√©moire).

#Pour la Vulgate
```{r}
for (word in tolower(df_Vulgate$lemma)) {
  if (!word %in% Stops) {
    cha√Æne_de_caract√®res_Vulgate <- paste(cha√Æne_de_caract√®res_Vulgate, word, sep=" ")
  }
  #J'enl√®ve la ponctuation.
  cha√Æne_de_caract√®res_Vulgate <- gsub("[[:punct:]]", "", cha√Æne_de_caract√®res_Vulgate)
}
cha√Æne_de_caract√®res_Vulgate
```

#Pour l'Heptateuque
```{r}
for (word in tolower(df_Heptateuque$lemma)) {
  if (!word %in% Stops) {
    cha√Æne_de_caract√®res_Heptateuque <- paste(cha√Æne_de_caract√®res_Heptateuque, word, sep=" ")
  }
  #J'enl√®ve la ponctuation.
  cha√Æne_de_caract√®res_Heptateuque <- gsub("[[:punct:]]", "", cha√Æne_de_caract√®res_Heptateuque)
}
cha√Æne_de_caract√®res_Heptateuque
```

  1.4. Cr√©ation d'une liste pour une approche _bag_of_words_

On divise le texte en une dizaine de morceaux (10 est un chiffre arbitraire, on peut en mettre plus ou moins) et on met ces dix morceaux dans une liste qu'on peut par exemple baptiser Extraits. C'est ce qui permet l'approche _bag_of_words_.

Approche _bag_of_words_ : id√©e que le monde peut √™tre d√©crit au moyen d'un dictionnaire. Dans sa version la plus simple, un document particulier est repr√©sent√© par l'histogramme des occurrences des mots le composant : pour un document donn√©, chaque mot se voit affect√© le nombre de fois qu'il appara√Æt dans le document (source : Wikip√©dia).

#Pour la Vulgate
```{r}
Nb_sequences <- 10
Extraits_Vulgate <- strwrap(cha√Æne_de_caract√®res_Vulgate, nchar(cha√Æne_de_caract√®res_Vulgate) / Nb_sequences)
#On peut afficher le contenu de chaque s√©quence :
Extraits_Vulgate[1]
```

#Pour l'Heptateuque
```{r}
Nb_sequences <- 10
Extraits_Heptateuque <- strwrap(cha√Æne_de_caract√®res_Heptateuque, nchar(cha√Æne_de_caract√®res_Heptateuque) / Nb_sequences)
Extraits_Heptateuque[3]
```

 1.5.Transformation en matrice vectorielle

Il faut installer les packages tm et tidytext.
#Pour la Vulgate
```{r}
if(!require("tm")){
  install.packages("tm")
  library("tm")
}
if(!require("tidytext")){
  install.packages("tidytext")
  library("tidytext")
}

#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des donn√©es et des m√©tadonn√©es.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_Vulgate <- Corpus(VectorSource(Extraits_Vulgate), readerControl = list(language = "lat"))
# J'affiche les informations √† propos de ce corpus
corpus_Vulgate
```

#Pour l'Heptateuque
```{r}
if(!require("tm")){
  install.packages("tm")
  library("tm")
}
if(!require("tidytext")){
  install.packages("tidytext")
  library("tidytext")
}

corpus_Heptateuque <- Corpus(VectorSource(Extraits_Heptateuque), readerControl = list(language = "lat"))

corpus_Heptateuque
```

 1.6 Cr√©ation d'un _document_term_matrix_

Un _document_term_matrix_ est une matrice math√©matique qui d√©crit la fr√©quence des termes qui apparaissent dans une collection de documents.

#Pour la Vulgate
```{r}
dtm_Vulgate <- DocumentTermMatrix(corpus_Vulgate)
dtm_Vulgate
```

#Pour l'Heptateuque
```{r}
dtm_Heptateuque <- DocumentTermMatrix(corpus_Heptateuque)
dtm_Heptateuque
```

#II. Analyse des donn√©es : fr√©quence des termes

 2.1.Graphe repr√©sentant la fr√©quence des termes
 Installation de la library pour le graphe et dessin du graphe

#Pour la Vulgate¬®
```{r}
freq_Vulgate <- as.data.frame(colSums(as.matrix(dtm_Vulgate)))
colnames(freq_Vulgate) <- c("frequence")
#as.data.frame est une fonction v√©rifiant qu'un objet est un dataframe ou le for√ßant √† le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction g√©n√©rique convertissant en matrice.
#colnames r√©cup√®re ou d√©finit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction g√©n√©rique qui combine ses arguments. La m√©thode par d√©faut combine les arguments pour former un vecteur.

#Pour dessiner un graphe, n√©cessit√© d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les donn√©es repr√©sent√©es graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
  install.packages("ggplot2")
  library("ggplot2")
}
#Dessin du graphe
#La fonction ggplot initialise le graphique. On commence par d√©finir la source des donn√©es (ici freq_Vulgate), puis on indique quelle donn√©e on veut repr√©senter (les attributs esth√©tiques) en passant des arguments dans la fonction aes(). Cette fonction sp√©cifie les variables √† visualiser et associe √† chaque variable un emplacement ou un r√¥le: on renseigne le param√®tre x qui est la variable √† repr√©senter sur l'axe horizontal (ici la fr√©quence).
#On ajoute, enfin, les √©l√©ments de repr√©sentation graphique (= geom). On les ajoute √† l'objet graphique de base avec l'op√©rateur +. geom_density permet d'afficher l'estimation de densit√© d'une variable num√©rique. On cr√©e une courbe de distribution.
#Source de la plupart des explications : https://juba.github.io/tidyverse/08-ggplot2.html
ggplot(freq_Vulgate, aes(x=frequence)) + geom_density()
```

#Pour l'Heptateuque
```{r}
freq_Heptateuque <- as.data.frame(colSums(as.matrix(dtm_Heptateuque)))
colnames(freq_Heptateuque) <- c("frequence")

#Dessin du graphe
ggplot(freq_Heptateuque, aes(x=frequence)) + geom_density()
```

 2.2 Analyse des donn√©es
 
 On retrouve la loi de Zipf dans la distribution des donn√©es.
 
 2.2.1 Mots avec de faibles fr√©quences
On peut compter les mots avec les fr√©quences faibles, par exemple avec moins de 10 occurrences (n+1).

#Pour la Vulgate
```{r}
motsPeuFrequents_Vulgate <- findFreqTerms(dtm_Vulgate, 0, 9)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsPeuFrequents)<-"latin-1"
length(motsPeuFrequents_Vulgate)
head(motsPeuFrequents_Vulgate,50)
```

#Pour l'Heptateuque
```{r}
#Je retire tous les mots qui apparaissent tr√®s peu.
motsPeuFrequents_Heptateuque<- findFreqTerms(dtm_Heptateuque, 0, 9)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsPeuFrequents)<-"latin-1"
length(motsPeuFrequents_Heptateuque)
head(motsPeuFrequents_Heptateuque,50)
```

 2.2.2 Mots avec de fortes fr√©quences
 On peut aussi compter et afficher les mots les plus fr√©quents, par exemple avec plus de 50 occurrences.

#Pour la Vulgate
```{r}
motsTresFrequents_Vulgate <- findFreqTerms(dtm_Vulgate, 49, Inf)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_Vulgate)
head(motsTresFrequents_Vulgate,50)
```

```{r}
motsTresFrequents_Vulgate <- findFreqTerms(dtm_Vulgate, 99, Inf)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_Vulgate)
head(motsTresFrequents_Vulgate,50)
```

#Pour l'Heptateuque
```{r}
motsTresFrequents_Heptateuque <- findFreqTerms(dtm_Heptateuque, 9, Inf)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_Heptateuque)
head(motsTresFrequents_Heptateuque,50)
```

```{r}
motsTresFrequents_Heptateuque <- findFreqTerms(dtm_Heptateuque, 11, Inf)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents_Heptateuque)
head(motsTresFrequents_Heptateuque,50)
```

 2.2.3.Association entre les mots
#Pour l'Heptateuque
```{r}
findAssocs(dtm_Heptateuque, terms = "dominus", corlimit = 0.5)
```
```{r}
findAssocs(dtm_Heptateuque, terms = "uir", corlimit = 0.5)
```
```{r}
findAssocs(dtm_Heptateuque, terms = "ius1", corlimit = 0.5)
```

 2.3 Nettoyage de la DTM pour √©liminer les rangs vides.

#Pour la Vulgate
```{r}
rowTotals <- apply(dtm_Vulgate, 1, sum)      #On trouve la somme des mots dans chaque document.
dtm_Vulgate_clean   <- dtm_Vulgate[rowTotals> 0, ]    #On retire tous les documents sans mot.
```

#Pour l'Heptateuque
```{r}
rowTotals <- apply(dtm_Heptateuque, 1, sum)
dtm_Heptateuque_clean   <- dtm_Heptateuque[rowTotals> 0, ]
```

#III. Topic Modeling

Un th√®me ( _topic_ ) est un _cluster_ de mots i.e. une r√©currence de co-occurrences.

3.1 Installation de la library pour le _topic_modeling_

Comme le package "topicmodels" ne parvenait pas √† s'installer, il a fallu t√©l√©charger la biblioth√®que GSL (biblioth√®que pour le calcul num√©rique en C et C++) via le terminal de l'ordinateur.

```{r}
if(!require("topicmodels")){
  install.packages("topicmodels")
  library("topicmodels")
}
```
 
 3.2 LDA ( _Latent Dirichlet allocation_ )
 
 La _LDA_  est un mod√®le g√©n√©ratif probabiliste permettant d‚Äôexpliquer des ensembles d‚Äôobservations au moyen de groupes non observ√©s, eux-m√™mes d√©finis par des similarit√©s de donn√©es. Le mod√®le va classer al√©atoirement tous les mots en _n_ sujets, et tenter d'affiner cette r√©partition de mani√®re it√©rative en observant les contextes.
Il faut d√©finir √† l'avance un nombre de sujets/th√®mes ( cf. infra la variable `k`).
 
#Pour la Vulgate
```{r}
#On peut partir sur une classification en deux _topics_.
k = 2
lda_2_Vulgate <- LDA(dtm_Vulgate_clean, k= k, control = list(seed = 1234))
#Seed doit √™tre un nombre al√©atoire.
#Puis on peut tenter une classification en trois topics.
lda_3_Vulgate <- LDA(dtm_Vulgate_clean, k= k+1, control = list(alpha = 0.1))
```

Le r√©sultat produit est une matrice avec pour chaque mot la probabilit√© qu'il appartienne √† l'un des diff√©rents _topics_. On donne un score _Œ≤_, qui est celui pr√©sent√© infra (il contient les probabilit√©s de chaque mot d‚Äôavoir √©t√© g√©n√©r√© par un topic).

```{r}
topics_2_Vulgate <- tidy(lda_2_Vulgate, matrix = "beta")
#Tidy est une fonction rangeant le r√©sultat d'un test dans un dataframe r√©capitulatif.
topics_2_Vulgate
```

```{r}
topics_3_Vulgate <- tidy(lda_3_Vulgate, matrix = "beta")
topics_3_Vulgate
```

#Pour l'Heptateuque
```{r}
#On peut partir sur une classification en deux _topics_.
k = 2
lda_2_Heptateuque <- LDA(dtm_Heptateuque_clean, k= k, control = list(seed = 1234))
#Seed doit √™tre un nombre al√©atoire.
#Puis on peut tenter une classification en trois topics.
lda_3_Heptateuque <- LDA(dtm_Heptateuque_clean, k= k+1, control = list(alpha = 0.1))
```

Le r√©sultat produit est une matrice avec pour chaque mot la probabilit√© qu'il appartienne √† un des diff√©rents _topics_. On donne un score _Œ≤_, qui est celui pr√©sent√© infra.

```{r}
topics_2_Heptateuque <- tidy(lda_2_Heptateuque, matrix = "beta")
topics_2_Heptateuque
```

```{r}
topics_3_Heptateuque <- tidy(lda_3_Heptateuque, matrix = "beta")
topics_3_Heptateuque
```

 3.3 Les param√®tres de Gibbs
 
Les param√®tres de Gibbs permettent une sophistication du syst√®me pr√©c√©dent. C'est une probabilit√© conditionnelle qui s'appuie, pour calculer le _Œ≤_ d'un mot, sur le _Œ≤_ des mots voisins. Pour ce faire, il faut d√©terminer:
1. √Ä quel point un document aime un _topic_.
2. √Ä quel point un _topic_ aime un mot.

 3.3.1 Installation de la library "ldatuning" pour d√©terminer le nombre optimal de topics
 
Pour installer cette library, il a √©t√© n√©cessaire de taper la commande suivante dans le terminal de l'ordinateur : sudo apt-get install libmpfr-dev (travail sous Linux).

```{r}
if(!require("ldatuning")){
  install.packages("ldatuning")
  library("ldatuning")
}
```

3.3.2 D√©termination du nombre optimal de topics.
#Pour la Vulgate
```{r}
#Ex√©cution du calcul avec la fonction FindTopicsNumber
topicsNumber_Vulgate <- FindTopicsNumber(
  #La DTM utilis√©e est la suivante :
  dtm_Vulgate_clean,
  #Le nombre de possibilit√©s test√©es :
  topics = seq(from = 2, to = 20, by = 1),
  #Les m√©triques utilis√©es
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE #Si c'est FALSE, cela supprime tous les avertissments et les informations additionnelles.
)

#Utilisation de la fonction seq()qui permet de cr√©er une s√©quence d'√©l√©ments dans un vecteur. La syntaxe est la suivante : seq (from, to, by, length.out) from = √©l√©ment de d√©but de la s√©quence ; to = √©l√©ment de fin de la s√©quence ; by = diff√©rence entre les √©l√©ments ; length.out = longueur maximale du vecteur.

#Affichage du r√©sultat
FindTopicsNumber_plot(topicsNumber_Vulgate)
#Lecture du graph : "‚ÄúGriffiths‚Äù et ‚ÄúDeveaud‚Äù suivent un principe de maximisation alors que ‚ÄúCaoJuan‚Äù et ‚ÄúArun‚Äù ob√©issent √† un principe de minimisation. Je vous √©pargne les d√©tails techniques, mais l‚Äôid√©e ici est d‚Äôidentifier l‚Äôendroit o√π simultan√©ment ‚ÄúGriffiths‚Äù et ‚ÄúDeveaud‚Äù se rejoignent le plus et o√π c‚Äôest √©galement le cas pour ‚ÄúCaoJuan‚Äù et ‚ÄúArun‚Äù. Tout est histoire de compromis, trouver l‚Äôendroit ou l‚Äô√©cart entre les courbes est minimal en haut et en bas !" (source : https://ouvrir.passages.cnrs.fr/wp-content/uploads/2019/07/rapp_topicmodel.html)
```
 Le nombre optimal de topics semble ici √™tre 7.
 
#Pour l'Heptateuque
```{r}
#Ex√©cution du calcul
topicsNumber_Heptateuque <- FindTopicsNumber(
  #La DTM utilis√©e
  dtm_Heptateuque_clean,
  #Le nombre de possibilit√©s test√©es
  topics = seq(from = 2, to = 20, by = 1),
  #Les m√©triques utilis√©es
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
#Affichage du r√©sultat
FindTopicsNumber_plot(topicsNumber_Heptateuque)
```
 Le nombre optimal de topics semble √™tre 6.
 
 3.3.3 Ex√©cution du calcul pour le topic modeling
#Pour la Vulgate
```{r}
## Set parameters for Gibbs sampling
#Le mod√®le va tourner 2000 fois avant de commencer √† enregistrer les r√©sultats
burnin <- 2000
#Apr√®s cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le r√©sultat que toutes les 500 it√©rations
thin <- 500
#seed et nstart pour la reproductibilit√©
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul le meilleur mod√®le est utilis√©.
best <- TRUE
#7 topics
lda_gibbs_7_Vulgate <- LDA(dtm_Vulgate_clean, 7, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#Utilisation de la fonction LDA avec la dtm utilis√©e, le nombre de topics, la m√©thode et le contr√¥le appliqu√©.

#19 topics
lda_gibbs_19_Vulgate <- LDA(dtm_Vulgate_clean, 19, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
```

On peut d√©sormais voir les premiers r√©sultats pour chacun des mod√®les. Il s'agit des mots dont la fr√©quence d'utilisation est corr√©l√©e.

```{r}
"LDA 2"
termsTopic_lda_2_Vulgate <- as.data.frame(terms(lda_2_Vulgate,10))
head(termsTopic_lda_2_Vulgate,11)
"LDA 3"
termsTopic_lda_3_Vulgate <- as.data.frame(terms(lda_3_Vulgate,10))
head(termsTopic_lda_3_Vulgate,11)
"LDA GIBBS 7"
termsTopic_lda_gibbs_7_Vulgate <- as.data.frame(terms(lda_gibbs_7_Vulgate,10))
head(termsTopic_lda_gibbs_7_Vulgate,11)
"LDA GIBBS 19"
termsTopic_lda_gibbs_19_Vulgate <- as.data.frame(terms(lda_gibbs_19_Vulgate,10))
head(termsTopic_lda_gibbs_19_Vulgate,11)
```

Nous allons utiliser `lda_gibbs_7_Vulgate`, comme le nombre optimal de topics √©tait de 7, et construire une matrice avec les _Œ≤_ des tokens (pour les …£, et donc des probabilit√©s par document, on aurait mis `matrix = "gamma"`). Chaque token est r√©p√©t√© deux fois, avec une probabilit√© pour chaque _topic_:

```{r}
topics_Vulgate <- tidy(lda_gibbs_7_Vulgate, matrix = "beta")
topics_Vulgate
```

#Pour l'Heptateuque
```{r}
## Set parameters for Gibbs sampling
#Le mod√®le va tourner 2000 fois avant de commencer √† enregistrer les r√©sultats
burnin <- 2000
#Apr√®s cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le r√©sultat que toutes les 500 it√©rations
thin <- 500
#seed et nstart pour la reproductibilit√©
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul le meilleur mod√®le est utilis√©.
best <- TRUE
#5 topics
lda_gibbs_5_Heptateuque <- LDA(dtm_Heptateuque_clean, 5, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#6 topics
lda_gibbs_6_Heptateuque <- LDA(dtm_Heptateuque_clean, 6, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
```

On peut d√©sormais voir les premiers r√©sultats pour chacun des mod√®les. Il s'agit des mots dont la fr√©quence d'utilisation est corr√©l√©e.

```{r}
"LDA 2"
termsTopic_lda_2_Heptateuque <- as.data.frame(terms(lda_2_Heptateuque,10))
head(termsTopic_lda_2_Heptateuque,11)
"LDA 3"
termsTopic_lda_3_Heptateuque <- as.data.frame(terms(lda_3_Heptateuque,10))
head(termsTopic_lda_3_Heptateuque,11)
"LDA GIBBS 5"
termsTopic_lda_gibbs_5_Heptateuque <- as.data.frame(terms(lda_gibbs_5_Heptateuque,10))
head(termsTopic_lda_gibbs_5_Heptateuque,11)
"LDA GIBBS 6"
termsTopic_lda_gibbs_6_Heptateuque <- as.data.frame(terms(lda_gibbs_6_Heptateuque,10))
head(termsTopic_lda_gibbs_6_Heptateuque,11)
```

Nous allons utiliser `lda_gibbs_6_Heptateuque`, comme 6 est apparu comme le nombre optimal possible de topics, et construire une matrice avec les _Œ≤_ des tokens (pour les …£, et donc des probabilit√©s par document, on aurait mis `matrix = "gamma"`). Chaque token est r√©p√©t√© deux fois, avec une probabilit√© pour chaque _topic_:

```{r}
topics_Heptateuque <- tidy(lda_gibbs_6_Heptateuque, matrix = "beta")
topics_Heptateuque
```

#IV. Visualisation

 4.1 R√©cup√©ration des mots

 4.1.1 Installation de la library "dplyr"
Cette library facilite le traitement et la manipulation de donn√©es contenues dans une ou plusieurs tables en proposant une syntaxe sous forme de verbes.
```{r}
if (!require("dplyr")){
   install.packages("dplyr")
  library("dplyr")
}
```

 4.1.2 Affichage des mots r√©cup√©r√©s dans un graphe
#Pour la Vulgate
```{r}
#Recup√©ration des mots
top_terms_Vulgate <- topics_Vulgate %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup()  %>%
  arrange(topic, -beta)
#Dessin du graphe
#On retrouve la fonction ggplot, cette fois-ci avec geom_col qui permet de cr√©er des diagrammes √† barres (barplots).
top_terms_Vulgate %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
                                                  facet_wrap(~ topic, scales = "free") +
                                                  coord_flip() +
                                                  scale_x_reordered()
```

#Pour l'Heptateuque
Avec 6 topics
```{r}
#R√©cup√©ration des mots
top_terms_Heptateuque_6 <- topics_Heptateuque %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup()  %>%
  arrange(topic, -beta)
#Dessin du graphe
top_terms_Heptateuque_6 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
                                                  facet_wrap(~ topic, scales = "free") +
                                                  coord_flip() +
                                                  scale_x_reordered()
```

 4.2 Association des tokens aux topics

Installation de la library reshape2 pour pouvoir utiliser la fonction melt qui permet de modifier le format des donn√©es en fonction d‚Äôune ou plusieurs variables de r√©f√©rence (passage d'une table large avec de nombreuses colonnes √† une table haute avec de nombreuses lignes et peu de colonnes).
```{r}
if (!require("reshape2")){
  install.packages("reshape2")
  library("reshape2")
}
```
 
#Pour la Vulgate
```{r}
df_Vulgate_2 <- melt(as.matrix(dtm_Vulgate_clean))
df_Vulgate_2 <- df_Vulgate_2[df_Vulgate_2$Terms %in%findFreqTerms(dtm_Vulgate_clean, lowfreq = 50), ]
ggplot(df_Vulgate_2, aes(as.factor(Docs), Terms, fill=log(value))) +
                                             geom_tile() +
                                             xlab("Sujets") +
                                             scale_fill_continuous(low="#FEE6CE", high="#E6550D") +
                                             theme(axis.text.x = element_text(angle=90, hjust=1))
```

```{r, fig.width=12, fig.height=12}
tt_Vulgate <- posterior(lda_gibbs_7_Vulgate)$terms
melted_Vulgate = melt(tt_Vulgate[,findFreqTerms(dtm_Vulgate_clean, 50,500)])

colnames(melted_Vulgate) <- c("Topics", "Terms", "value")
melted_Vulgate$Topics <- as.factor(melted_Vulgate$Topics)
ggplot(data = melted_Vulgate, aes(x=Topics, y=Terms, fill=value)) +
                                                                      geom_tile() +
                                                             theme(text = element_text(size=35))
```

#Pour l'Heptateuque
```{r}
df_Heptateuque <- melt(as.matrix(dtm_Heptateuque_clean))
df_Heptateuque <- df_Heptateuque[df_Heptateuque$Terms %in%findFreqTerms(dtm_Heptateuque_clean, lowfreq = 7), ]
ggplot(df_Heptateuque, aes(as.factor(Docs), Terms, fill=log(value))) +
                                             geom_tile() +
                                             xlab("Sujets") +
                                             scale_fill_continuous(low="#FEE6CE", high="#E6550D") +
                                             theme(axis.text.x = element_text(angle=90, hjust=1))
```

Avec 6 topics
```{r, fig.width=12, fig.height=12}
tt_Heptateuque <- posterior(lda_gibbs_6_Heptateuque)$terms
melted_Heptateuque = melt(tt_Heptateuque[,findFreqTerms(dtm_Heptateuque_clean, 10,20)])

colnames(melted_Heptateuque) <- c("Topics", "Terms", "value")
melted_Heptateuque$Topics <- as.factor(melted_Heptateuque$Topics)
ggplot(data = melted_Heptateuque, aes(x=Topics, y=Terms, fill=value)) + 
                                              geom_tile() +
                                              theme(text = element_text(size=35))
```

 4.3 Observation du _score gamma_
 
Le score gamma est la probabilit√© qu'un document contienne un sujet.

#Pour la Vulgate
```{r}
DocumentTopicProbabilities_Vulgate <- as.data.frame(lda_gibbs_7_Vulgate@gamma)
rownames(DocumentTopicProbabilities_Vulgate) <- rownames(corpus_Vulgate)
head(DocumentTopicProbabilities_Vulgate)
```

#Pour l'Heptateuque
```{r}
DocumentTopicProbabilities_Heptateuque <- as.data.frame(lda_gibbs_6_Heptateuque@gamma)
rownames(DocumentTopicProbabilities_Heptateuque) <- rownames(corpus_Heptateuque)
head(DocumentTopicProbabilities_Heptateuque)
```

 4.4. Nuages de mots
 
Pour faire des faire des _word clouds_, il faut installer les libraries suivantes :
```{r}
if (!require("wordcloud")){
   install.packages("wordcloud")
  library("wordcloud")
}
if (!require("RColorBrewer")){
   install.packages("RColorBrewer")
  library("RColorBrewer")
}
if (!require("wordcloud2")){
   install.packages("wordcloud2")
  library("wordcloud2")
}
```

#Pour la Vulgate
On r√©cup√®re les mots et on les associe √† leur ùõÉ

```{r, fig.width=20, fig.height=20}
tm_Vulgate <- posterior(lda_gibbs_7_Vulgate)$terms
data_Vulgate = data.frame(colnames(tm_Vulgate))
head(data_Vulgate)
```


Puis on produit une visualisation par _topic_

```{r, fig.width=30, fig.height=20}
for(topic in seq(from = 1, to = 7, by = 1)){
    data_Vulgate$topic <-tm_Vulgate[topic,]
    #text(x=0.5, y=1, paste("V",topic, sep=""),cex=0.6)
    wordcloud(
      words = data_Vulgate$colnames.tm_Vulgate., #Mots √† dessiner
      freq = data_Vulgate$topic, #Fr√©quence des mots
      #Min.freq=sous ce seuil, les mots ne seront pas affich√©s
      min.freq=0.0002,
      #max.words=nombre maximum de mots √† afficher
      max.words=20,
      #Random.order dessine les mots dans un ordre al√©atoire. Si faux, ils sont dessin√©s par ordre d√©croissant de la fr√©quence.
      random.order=FALSE,
      #rot.per=% de mots √† 90¬∞
      rot.per=.35,
      #taille du graphe
      scale=c(10,10),
      #couleurs
      colors = brewer.pal(5, "Dark2")
      # il est possible de rentrer directement les couleurs qui nous int√©ressent
      #c("red", "blue", "yellow", "chartreuse", "cornflowerblue", "darkorange")
    )
}
```


#Pour l'Heptateuque
On r√©cup√®re les mots et on les associe √† leur ùõÉ. On s'appuie sur le nombre de 6 topics.

```{r, fig.width=20, fig.height=20}
tm_Heptateuque <- posterior(lda_gibbs_6_Heptateuque)$terms
data_Heptateuque = data.frame(colnames(tm_Heptateuque))
head(data_Heptateuque)
```

Puis on produit une visualisation par _topic_

```{r, fig.width=30, fig.height=20}
for(topic in seq(from = 1, to = 6, by = 1)){
    data_Heptateuque$topic <-tm_Heptateuque[topic,]
    #text(x=0.5, y=1, paste("V",topic, sep=""),cex=0.6)
    wordcloud(
      words = data_Heptateuque$colnames.tm_Heptateuque.,
      freq = data_Heptateuque$topic,
      #sous ce seuil, les mots ne seront pas affich√©s
      min.freq=0.0002,
      #nombre maximum de mots √† afficher
      max.words=30,
      #Si faux, en ordre croissant
      random.order=FALSE,
      #% de mots √† 90¬∞
      rot.per=.35,
      #taille du graph
      scale=c(10,10),
      #couleurs
      colors = brewer.pal(5, "Dark2")
      # il est possible de rentrer directement les couleurs qui nous int√©ressent
      #c("red", "blue", "yellow", "chartreuse", "cornflowerblue", "darkorange")
    )
}
```

# V. Sources

La grande majorit√© du code provient d'un cours de Simon Gabay (Universit√© de Gen√®ve).

